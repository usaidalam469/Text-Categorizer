{"cells":[{"cell_type":"code","execution_count":null,"id":"95e8bc29","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2666,"status":"ok","timestamp":1733284480094,"user":{"displayName":"Mohammad Usaid Syed","userId":"05566789139839224321"},"user_tz":0},"id":"95e8bc29","outputId":"c2cf7fd5-0a0f-4dd2-a414-a5cf3624d261"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: psycopg2 in /usr/local/lib/python3.10/dist-packages (2.9.10)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n","Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"]}],"source":["!pip install psycopg2 pandas"]},{"cell_type":"markdown","source":["# **IMPORTS**"],"metadata":{"id":"cLlq7yAmTOfP"},"id":"cLlq7yAmTOfP"},{"cell_type":"code","execution_count":1,"id":"UAZD4WJ9TdwQ","metadata":{"executionInfo":{"elapsed":26212,"status":"ok","timestamp":1734627963375,"user":{"displayName":"Mohammad Usaid Syed","userId":"11364143280426579494"},"user_tz":0},"id":"UAZD4WJ9TdwQ"},"outputs":[],"source":["import psycopg2\n","import pandas as pd\n","import re\n","from sklearn.model_selection import train_test_split\n","import spacy\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","import re\n","from datetime import datetime\n","\n","spacy.prefer_gpu()\n","\n","import os\n","from tqdm import tqdm\n","from spacy.tokens import DocBin\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","import locale\n","locale.getpreferredencoding = lambda: \"UTF-8\"\n","\n","from google.colab import drive\n","\n","import spacy\n","from spacy import displacy\n"]},{"cell_type":"code","execution_count":2,"id":"EahGgvpOh28K","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22887,"status":"ok","timestamp":1734627986232,"user":{"displayName":"Mohammad Usaid Syed","userId":"11364143280426579494"},"user_tz":0},"id":"EahGgvpOh28K","outputId":"8ba388ec-d093-4bd0-db37-9d7a2f320585"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Mount Google Drive\n","drive.mount('/content/drive')\n","drive_path = '/content/drive/MyDrive/AndElementTask/And Elements/'"]},{"cell_type":"markdown","source":["# **FETCHING DATA FOR DB**"],"metadata":{"id":"dVPh_qneTTNh"},"id":"dVPh_qneTTNh"},{"cell_type":"code","execution_count":11,"id":"4fef81e2","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10281,"status":"ok","timestamp":1734628272519,"user":{"displayName":"Mohammad Usaid Syed","userId":"11364143280426579494"},"user_tz":0},"id":"4fef81e2","outputId":"6c73b262-d6b1-4d98-d453-b8b7d663097e"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-11-b0678ea13553>:45: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n","  keywords = pd.read_sql_query(sql_query_keywords, connection)\n","<ipython-input-11-b0678ea13553>:46: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n","  articles = pd.read_sql_query(sql_query_articles, connection)\n"]},{"output_type":"stream","name":"stdout","text":["Data successfully exported!\n"]}],"source":["# PostgreSQL database connection details\n","db_params = {\n","    \"host\": \"viaduct.proxy.rlwy.net\",\n","    \"port\": 52900,\n","    \"database\": \"railway\",\n","    \"user\": \"postgres\",\n","    \"password\": \"D*eeE12*6gE*BF-G*E33cccfEb*2CB*c\"\n","}\n","\n","# SQL query to fetch data\n","sql_query_keywords = \"\"\"\n","select\n","    c.id, cat.category_name as keyword, cat.category_type as category\n","from\n","    public.categories cat\n","inner join\n","    public.category_article_relationship car\n","    on cat.id = car.category_id\n","inner join\n","    public.\"content\" c\n","    on c.id = car.article_id\n","where c.is_reviewed = true\n","order by c.id\n","\n","\"\"\"\n","\n","sql_query_articles = \"\"\"\n","select\n","    c.id, c.title || ' ' || c.\"content\" || ' ' || c.author AS content\n","from\n","    public.\"content\" c\n","where c.is_reviewed = true\n","order by c.id\n","\"\"\"\n","\n","# Output CSV file path\n","keywords_csv_path = \"keywords.csv\"\n","articles_csv_path = \"articles.csv\"\n","\n","try:\n","    # Connect to the PostgreSQL database\n","    connection = psycopg2.connect(**db_params)\n","\n","    # Use pandas to fetch the data\n","    keywords = pd.read_sql_query(sql_query_keywords, connection)\n","    articles = pd.read_sql_query(sql_query_articles, connection)\n","\n","    # Export the DataFrame to a CSV file\n","    keywords.to_csv(keywords_csv_path, index=False,encoding=\"utf-8\")\n","    articles.to_csv(articles_csv_path, index=False,encoding=\"utf-8\")\n","\n","    print(f\"Data successfully exported!\")\n","\n","except Exception as e:\n","    print(f\"Error: {e}\")\n","\n","finally:\n","    # Close the database connection\n","    if connection:\n","        connection.close()\n"]},{"cell_type":"code","execution_count":12,"id":"83478fa0","metadata":{"executionInfo":{"elapsed":737,"status":"ok","timestamp":1734628273249,"user":{"displayName":"Mohammad Usaid Syed","userId":"11364143280426579494"},"user_tz":0},"id":"83478fa0"},"outputs":[],"source":["# Load CSV files\n","keywords_df = pd.read_csv(\"keywords.csv\",encoding=\"utf-8\")\n","articles_df = pd.read_csv(\"articles.csv\",encoding=\"utf-8\")\n","\n","# Merge the two datasets on the \"id\" column\n","merged_df = pd.merge(keywords_df, articles_df, on=\"id\")"]},{"cell_type":"code","execution_count":13,"id":"HWX1quyruy1o","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":457,"status":"ok","timestamp":1734628286225,"user":{"displayName":"Mohammad Usaid Syed","userId":"11364143280426579494"},"user_tz":0},"id":"HWX1quyruy1o","outputId":"0b8af8e2-fcea-42ee-f34f-17ab6a0c57d3"},"outputs":[{"output_type":"stream","name":"stdout","text":["           id               keyword       category  \\\n","0           1           feb 18 2024          Crime   \n","1           1           jan 12 2024          Crime   \n","2           1                 hosts           Tags   \n","3           1                voting           Tags   \n","4           1                  fans           Tags   \n","...       ...                   ...            ...   \n","367704  21365               zhoresh     Characters   \n","367705  21365  book recommendations  Type_of_Story   \n","367706  21365      nuclear disaster           Tags   \n","367707  21365  andrew leatherbarrow     Characters   \n","367709  21365  absolutely essential       Sentence   \n","\n","                                                  content  \n","0       The 2024 People's Choice Awards Host and Nomin...  \n","1       The 2024 People's Choice Awards Host and Nomin...  \n","2       The 2024 People's Choice Awards Host and Nomin...  \n","3       The 2024 People's Choice Awards Host and Nomin...  \n","4       The 2024 People's Choice Awards Host and Nomin...  \n","...                                                   ...  \n","367704  All The Books The Creator Of HBO's 'Chernobyl'...  \n","367705  All The Books The Creator Of HBO's 'Chernobyl'...  \n","367706  All The Books The Creator Of HBO's 'Chernobyl'...  \n","367707  All The Books The Creator Of HBO's 'Chernobyl'...  \n","367709  All The Books The Creator Of HBO's 'Chernobyl'...  \n","\n","[303163 rows x 4 columns]\n"]}],"source":["# Drop rows with any NaN (null) values\n","merged_df = merged_df.dropna()\n","\n","# Filter rows\n","merged_df = merged_df[merged_df['keyword'].apply(lambda x: len(str(x).split()) <= 3)]\n","\n","# Output the filtered dataframe\n","print(merged_df)"]},{"cell_type":"code","execution_count":14,"id":"iKIGEMX6bBUC","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":347,"status":"ok","timestamp":1734628287663,"user":{"displayName":"Mohammad Usaid Syed","userId":"11364143280426579494"},"user_tz":0},"id":"iKIGEMX6bBUC","outputId":"51f9e0f3-9680-431e-d29e-2452ad474bbe"},"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","Index: 303163 entries, 0 to 367709\n","Data columns (total 4 columns):\n"," #   Column    Non-Null Count   Dtype \n","---  ------    --------------   ----- \n"," 0   id        303163 non-null  int64 \n"," 1   keyword   303163 non-null  object\n"," 2   category  303163 non-null  object\n"," 3   content   303163 non-null  object\n","dtypes: int64(1), object(3)\n","memory usage: 11.6+ MB\n"]}],"source":["merged_df.info()"]},{"cell_type":"markdown","source":["# **HELPER FUNCTIONS**"],"metadata":{"id":"R-KDMuhUUOaW"},"id":"R-KDMuhUUOaW"},{"cell_type":"code","execution_count":15,"id":"il5JZ5RI_gGn","metadata":{"executionInfo":{"elapsed":311,"status":"ok","timestamp":1734628289090,"user":{"displayName":"Mohammad Usaid Syed","userId":"11364143280426579494"},"user_tz":0},"id":"il5JZ5RI_gGn"},"outputs":[],"source":["def preprocess_text(text):\n","    \"\"\"\n","    Preprocess the text by:\n","    - Replacing periods in emails and URLs with spaces.\n","    - Replacing special characters (!, +, -, @, em dashes, quotation marks) with spaces.\n","    - Handling URLs (e.g., http://example becomes http example).\n","    - Removing special characters between words.\n","    - Removing trailing spaces and converting text to lowercase.\n","    - Removing all stop words.\n","\n","    Args:\n","    - text (str): The input text to preprocess.\n","\n","    Returns:\n","    - str: The cleaned text\n","    \"\"\"\n","    text = str(text)  # Ensure the input is a string\n","\n","    # Handle URLs by replacing \":\" and \"/\" with spaces\n","    text = re.sub(r\"http[s]?://\", \"http \", text)  # Replace http:// or https:// with \"http\"\n","    text = re.sub(r\"[/:]\", \" \", text)  # Replace remaining \":\" and \"/\" with spaces\n","\n","    # Remove periods in emails and URLs\n","    text = re.sub(r\"(?<=\\S)\\.(?=\\S)\", \" \", text)\n","\n","    # Replace specific special characters (!, +, -, @, em dash, quotation marks, etc.) with spaces\n","    text = re.sub(r\"[!+\\-@“”\\\"‘’—–]\", \" \", text)\n","\n","    # Replace special characters between words\n","    text = re.sub(r\"(?<=\\w)[^\\w\\s,](?=\\w)\", \" \", text)\n","\n","    # Convert to lowercase\n","    text = text.lower()\n","\n","    # Remove extra spaces and trim leading/trailing spaces\n","    text = re.sub(r\"\\s+\", \" \", text).strip()\n","\n","    return text\n","\n","def split_text_into_chunks(text, chunk_size=500, overlap=50):\n","    \"\"\"\n","    Split the text into chunks of specified size with overlap.\n","\n","    Args:\n","    - text (str): The input text.\n","\n","    Returns:\n","    - str: Chunks\n","    \"\"\"\n","    words = text.split()\n","    chunks = []\n","    for i in range(0, len(words), chunk_size - overlap):\n","        chunk = \" \".join(words[i:i + chunk_size])\n","        chunks.append(chunk)\n","        if i + chunk_size >= len(words):\n","            break\n","    return chunks"]},{"cell_type":"code","source":["def extract_sentences_with_keywords(article, keywords):\n","    \"\"\"\n","    Extract sentences containing any of the given keywords and their surrounding sentences (±1 sentence).\n","\n","    Args:\n","    - article (str): The full article text.\n","    - keywords (list): List of keywords to look for.\n","\n","    Returns:\n","    - str: Filtered article with relevant sentences and their context.\n","    \"\"\"\n","    # Split the article into sentences using regex for sentence boundaries\n","    sentences = re.split(r'(?<=[.!?])\\s+', article)\n","\n","    # Find sentences with any of the keywords\n","    keyword_indices = [\n","        idx for idx, sentence in enumerate(sentences)\n","        if any(re.search(rf\"\\b{re.escape(keyword)}\\b\", sentence, flags=re.IGNORECASE) for keyword in keywords)\n","    ]\n","\n","    # Include surrounding sentences (±1 sentence for context)\n","    relevant_indices = set()\n","    for idx in keyword_indices:\n","        relevant_indices.update([idx - 1, idx, idx + 1])  # Include previous, current, and next sentences\n","\n","    # Ensure indices are within valid range\n","    relevant_indices = sorted(idx for idx in relevant_indices if 0 <= idx < len(sentences))\n","\n","    # Extract relevant sentences\n","    filtered_sentences = [sentences[idx] for idx in relevant_indices]\n","\n","    return \" \".join(filtered_sentences)\n","\n","def create_annotations_with_context_and_chunks(merged_df, chunk_size=512, overlap=50):\n","    \"\"\"\n","    Create training annotations using relevant sentences with keywords and their surrounding context,\n","    while breaking the filtered text into manageable chunks.\n","\n","    Args:\n","    - merged_df (pd.DataFrame): DataFrame containing 'id', 'cleaned_content', 'cleaned_keyword', and 'category'.\n","    - chunk_size (int): Maximum size (in characters) for each chunk of filtered text.\n","\n","    Returns:\n","    - tuple: (training_data, label_missing_counts, label_correct_counts)\n","    \"\"\"\n","    training_data = []\n","    label_missing_counts = {label: 0 for label in merged_df[\"category\"].unique()}  # Initialize counts for labels\n","    label_correct_counts = {label: 0 for label in merged_df[\"category\"].unique()}\n","\n","    for article_id, group in merged_df.groupby(\"id\"):\n","        try:\n","            full_text = group[\"cleaned_content\"].iloc[0]  # Full article text\n","            keywords = group[\"cleaned_keyword\"].tolist()  # List of keywords for the article\n","            categories = group[\"category\"].tolist()  # Corresponding categories for the keywords\n","\n","            # Extract relevant sentences using keywords\n","            filtered_text = extract_sentences_with_keywords(full_text, keywords)\n","\n","            # Split the filtered text into chunks\n","            chunks = split_text_into_chunks(filtered_text, chunk_size, overlap)\n","\n","            for chunk in chunks:\n","                entities = []\n","                has_keyword = False\n","\n","                for keyword, label in zip(keywords, categories):\n","                    # Check if the keyword exists in the current chunk\n","                    if re.search(r'\\b' + re.escape(keyword) + r'\\b', chunk, flags=re.IGNORECASE):\n","                        has_keyword = True\n","                        for match in re.finditer(r'\\b' + re.escape(keyword) + r'\\b', chunk, flags=re.IGNORECASE):\n","                            start = match.start()\n","                            end = match.end()\n","                            label_correct_counts[label] += 1\n","                            entities.append((start, end, label))\n","                    else:\n","                        label_missing_counts[label] += 1\n","\n","                if has_keyword:\n","                    training_data.append((chunk, {\"entities\": entities}))\n","\n","        except Exception as e:\n","            print(f\"Error processing article ID {article_id}: {e}\")\n","\n","    return training_data, label_missing_counts, label_correct_counts\n","\n","def create_annotations_with_chunks_single_entity(merged_df, chunk_size=500, overlap=50):\n","    training_data = []\n","    label_missing_counts = {label: 0 for label in merged_df[\"category\"].unique()}  # Initialize counts for labels\n","    label_correct_counts = {label: 0 for label in merged_df[\"category\"].unique()}\n","\n","    for article_id, group in merged_df.groupby(\"id\"):\n","        try:\n","            text = group[\"cleaned_content\"].iloc[0]\n","            chunks = split_text_into_chunks(text, chunk_size, overlap)  # Split text into chunks\n","\n","            for chunk in chunks:\n","                has_keyword = False\n","                entities = []\n","                for _, row in group.iterrows():\n","                    keyword = row[\"cleaned_keyword\"]\n","                    label = row[\"category\"]\n","\n","                    # Check if the keyword exists in the chunk\n","                    if re.search(r'\\b' + re.escape(keyword) + r'\\b', chunk, flags=re.IGNORECASE):\n","                        has_keyword = True\n","                        for match in re.finditer(r'\\b' + re.escape(keyword) + r'\\b', chunk, flags=re.IGNORECASE):\n","                            start = match.start()\n","                            end = match.end()\n","                            entities.append((start, end, label))\n","                            break\n","\n","                if has_keyword:\n","                  label_correct_counts[label] += 1\n","                  training_data.append((chunk, {\"entities\": entities}))\n","                else:\n","                  label_missing_counts[label] += 1\n","                  training_data.append((chunk, {\"entities\": []}))\n","\n","        except Exception as e:\n","            print(f\"Error processing article ID {article_id}: {e}\")\n","\n","    return training_data, label_missing_counts, label_correct_counts\n","\n","# Main function to create annotations\n","def create_annotations_with_chunks(merged_df, chunk_size=500, overlap=50):\n","    training_data = []\n","    label_missing_counts = {label: 0 for label in merged_df[\"category\"].unique()}  # Initialize counts for labels\n","    label_correct_counts = {label: 0 for label in merged_df[\"category\"].unique()}\n","\n","    for article_id, group in merged_df.groupby(\"id\"):\n","        try:\n","            text = group[\"cleaned_content\"].iloc[0]\n","            chunks = split_text_into_chunks(text, chunk_size, overlap)  # Split text into chunks\n","\n","            for chunk in chunks:\n","                has_keyword = False\n","                entities = []\n","                for _, row in group.iterrows():\n","                    keyword = row[\"cleaned_keyword\"]\n","                    label = row[\"category\"]\n","\n","                    # Check if the keyword exists in the chunk\n","                    if re.search(r'\\b' + re.escape(keyword) + r'\\b', chunk, flags=re.IGNORECASE):\n","                        has_keyword = True\n","                        for match in re.finditer(r'\\b' + re.escape(keyword) + r'\\b', chunk, flags=re.IGNORECASE):\n","                            start = match.start()\n","                            end = match.end()\n","                            label_correct_counts[label] += 1\n","                            entities.append((start, end, label))\n","                            break\n","                    else:\n","                        label_missing_counts[label] += 1\n","\n","                if has_keyword:\n","                    training_data.append((chunk, {\"entities\": entities}))\n","\n","        except Exception as e:\n","            print(f\"Error processing article ID {article_id}: {e}\")\n","\n","    return training_data, label_missing_counts, label_correct_counts\n","\n","def create_annotations(merged_df):\n","    training_data = []\n","    label_missing_counts = {label: 0 for label in merged_df[\"category\"].unique()}  # Initialize counts for labels\n","    label_correct_counts = {label: 0 for label in merged_df[\"category\"].unique()}\n","\n","    for article_id, group in merged_df.groupby(\"id\"):\n","        try:\n","            text = group[\"cleaned_content\"].iloc[0]  # Full article text\n","            entities = []\n","            has_keyword = False\n","\n","            for _, row in group.iterrows():\n","                keyword = row[\"cleaned_keyword\"]\n","                label = row[\"category\"]\n","\n","                # Check if the keyword exists in the article\n","                if re.search(r'\\b' + re.escape(keyword) + r'\\b', text, flags=re.IGNORECASE):\n","                    has_keyword = True\n","                    for match in re.finditer(r'\\b' + re.escape(keyword) + r'\\b', text, flags=re.IGNORECASE):\n","                        start = match.start()\n","                        end = match.end()\n","                        label_correct_counts[label] += 1\n","                        entities.append((start, end, label))\n","                else:\n","                    label_missing_counts[label] += 1\n","\n","            if has_keyword:\n","                training_data.append((text, {\"entities\": entities}))\n","\n","        except Exception as e:\n","            print(f\"Error processing article ID {article_id}: {e}\")\n","\n","    return training_data, label_missing_counts, label_correct_counts\n","\n","def save_training_data_to_csv(training_data, output_file):\n","    \"\"\"\n","    Save training data to a CSV file in the format:\n","    article_id, article, Entities\n","\n","    Args:\n","    - training_data (list): A list of tuples containing text and entity annotations.\n","    - output_file (str): The path to save the CSV file.\n","    \"\"\"\n","    data_rows = []\n","    for article_id, (text, annotation) in enumerate(training_data):\n","        # Extract the entities as a list of tuples (start, end, label)\n","        entities = [(start, end, label) for start, end, label in annotation[\"entities\"]]\n","\n","        # Prepare a dictionary for the row\n","        data_rows.append({\n","            \"article_id\": article_id,\n","            \"article\": text,\n","            \"Entities\": str(entities)  # Convert list of tuples to string format\n","        })\n","\n","    # Convert to DataFrame\n","    df = pd.DataFrame(data_rows)\n","\n","    # Save to CSV\n","    df.to_csv(output_file, index=False)\n","    print(f\"Training data saved to {output_file}\")"],"metadata":{"id":"urrrGQBwTmqF","executionInfo":{"status":"ok","timestamp":1734628296274,"user_tz":0,"elapsed":279,"user":{"displayName":"Mohammad Usaid Syed","userId":"11364143280426579494"}}},"id":"urrrGQBwTmqF","execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["# **DATA PREPROCESSING**"],"metadata":{"id":"FOa_F_8HWI-q"},"id":"FOa_F_8HWI-q"},{"cell_type":"code","execution_count":17,"id":"172fc80c","metadata":{"id":"172fc80c","executionInfo":{"status":"ok","timestamp":1734628676519,"user_tz":0,"elapsed":378109,"user":{"displayName":"Mohammad Usaid Syed","userId":"11364143280426579494"}}},"outputs":[],"source":["# Apply preprocessing to the 'content' column\n","merged_df['cleaned_content'] = merged_df['content'].apply(preprocess_text)\n","# Apply preprocessing to the 'keyword' column\n","merged_df['cleaned_keyword'] = merged_df['keyword'].apply(preprocess_text)"]},{"cell_type":"code","source":["filtered_df = merged_df[merged_df['id'] < 2000]\n","len(filtered_df)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cbu3MY9WMUkZ","executionInfo":{"status":"ok","timestamp":1734628768451,"user_tz":0,"elapsed":249,"user":{"displayName":"Mohammad Usaid Syed","userId":"11364143280426579494"}},"outputId":"b81fa92c-c02f-48ef-9c87-6c24a824d20e"},"id":"cbu3MY9WMUkZ","execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["48364"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["# **FILTERING DATA**"],"metadata":{"id":"Pu-e_HEFW1HQ"},"id":"Pu-e_HEFW1HQ"},{"cell_type":"code","source":["tags_df = merged_df[merged_df['category'] == 'Tags']\n","\n","# Count the frequency of each keyword in the cleaned_keyword column\n","keyword_counts = tags_df['keyword'].value_counts()\n","# Convert to a DataFrame for easier handling\n","keyword_counts_df = keyword_counts.reset_index()\n","keyword_counts_df.columns = ['keyword', 'count']\n","keyword_counts_df"],"metadata":{"id":"RkQOe2coWzzv"},"id":"RkQOe2coWzzv","execution_count":null,"outputs":[]},{"cell_type":"code","source":["setting_df = merged_df[merged_df['category'] == 'Setting']\n","\n","# Count the frequency of each keyword in the cleaned_keyword column\n","keyword_counts = setting_df['keyword'].value_counts()\n","# Convert to a DataFrame for easier handling\n","keyword_counts_df_2 = keyword_counts.reset_index()\n","keyword_counts_df_2.columns = ['keyword', 'count']\n","keyword_counts_df_2"],"metadata":{"id":"yi7gtsNLW3_q"},"id":"yi7gtsNLW3_q","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Filter keywords\n","filtered_tags_keywords = keyword_counts_df[keyword_counts_df['count'] > 100]['keyword']\n","filtered_setting_keywords = keyword_counts_df_2[keyword_counts_df_2['count'] > 20]['keyword']\n","\n","# Filter tags_df to keep only rows with the filtered keywords\n","filtered_df = merged_df[(merged_df['keyword'].isin(filtered_setting_keywords) & (merged_df['category'] == 'Setting')) | (merged_df['keyword'].isin(filtered_tags_keywords) & (merged_df['category'] == 'Tags'))]\n","\n","# Display the filtered dataframe\n","print(filtered_df)"],"metadata":{"id":"2WgTYy33W-vQ"},"id":"2WgTYy33W-vQ","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **CREATE ANNOTATIONS WITH CHUNKS**"],"metadata":{"id":"EZ4cUxO3WbL9"},"id":"EZ4cUxO3WbL9"},{"cell_type":"code","execution_count":19,"id":"K78bUt2Xxrtg","metadata":{"id":"K78bUt2Xxrtg","executionInfo":{"status":"ok","timestamp":1734628802200,"user_tz":0,"elapsed":26961,"user":{"displayName":"Mohammad Usaid Syed","userId":"11364143280426579494"}}},"outputs":[],"source":["training_data, label_missing_counts, label_correct_counts = create_annotations_with_context_and_chunks(filtered_df)"]},{"cell_type":"code","source":["training_data, label_missing_counts, label_correct_counts = create_annotations_with_chunks_single_entity(filtered_df)"],"metadata":{"id":"nwCiWSUMWhDA"},"id":"nwCiWSUMWhDA","execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_data, label_missing_counts, label_correct_counts = create_annotations_with_chunks(filtered_df)"],"metadata":{"id":"r01jU3LgWqKl"},"id":"r01jU3LgWqKl","execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_data, label_missing_counts, label_correct_counts = create_annotations(filtered_df)"],"metadata":{"id":"RUrCCQPUWsxL"},"id":"RUrCCQPUWsxL","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Plotting Annotations"],"metadata":{"id":"FF9Ko0VfZ-bl"},"id":"FF9Ko0VfZ-bl"},{"cell_type":"code","source":["# Combine all unique labels from both dictionaries\n","all_labels = set(label_correct_counts.keys()).union(label_missing_counts.keys())\n","\n","# Ensure all labels have counts in both dictionaries (default to 0 if missing)\n","correct_counts = [label_correct_counts.get(label, 0) for label in all_labels]\n","missing_counts = [label_missing_counts.get(label, 0) for label in all_labels]\n","\n","# Bar plot configuration\n","x = np.arange(len(all_labels))  # Label locations\n","width = 0.5  # Bar width\n","\n","# Create the plot\n","fig, ax = plt.subplots(figsize=(15, 8))\n","bars_correct = ax.bar(x - width / 2, correct_counts, width, label='Correct Counts', color='green')\n","bars_missing = ax.bar(x + width / 2, missing_counts, width, label='Missing Counts', color='red')\n","\n","# Add labels, title, and legend\n","ax.set_xlabel('Labels', fontsize=12)\n","ax.set_ylabel('Counts', fontsize=12)\n","ax.set_title('Comparison of Correct and Missing Counts by Label', fontsize=14)\n","ax.set_xticks(x)\n","ax.set_xticklabels(all_labels, rotation=45, ha=\"right\", fontsize=12)\n","ax.legend()\n","\n","# Display counts on top of bars\n","def add_labels(bars):\n","    for bar in bars:\n","        height = bar.get_height()\n","        ax.annotate(f'{height}',\n","                    xy=(bar.get_x() + bar.get_width() / 2, height),\n","                    xytext=(0, 2),  # Offset text above bar\n","                    textcoords=\"offset points\",\n","                    ha='center', va='bottom')\n","\n","add_labels(bars_correct)\n","add_labels(bars_missing)\n","\n","# Show the plot\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"2C0WQI6xZ8Bj"},"id":"2C0WQI6xZ8Bj","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **CHECKING SPANS**"],"metadata":{"id":"8wBcDjZBX0Yp"},"id":"8wBcDjZBX0Yp"},{"cell_type":"code","source":["# Removing overlapping spans\n","for text, annotations in training_data:\n","    spans = annotations[\"entities\"]\n","    # Sort the spans by their start position\n","    spans = sorted(spans, key=lambda span: span[0])\n","\n","    # List to hold non-overlapping spans\n","    non_overlapping_spans = []\n","\n","    for span in spans:\n","        # If non_overlapping_spans is empty, add the first span directly\n","        if not non_overlapping_spans:\n","            non_overlapping_spans.append(span)\n","        else:\n","            # Get the last span in the non_overlapping_spans list\n","            last_span = non_overlapping_spans[-1]\n","\n","            # Check for overlap with the last span\n","            if last_span[1] > span[0]:\n","                # If overlapping, keep the span with the greater length\n","                if (last_span[1] - last_span[0]) < (span[1] - span[0]):\n","                    non_overlapping_spans[-1] = span  # Replace with the longer span\n","            else:\n","                # If no overlap, add the span to the list\n","                non_overlapping_spans.append(span)\n","\n","    # Replace the original spans with the filtered non-overlapping spans\n","    annotations[\"entities\"] = non_overlapping_spans"],"metadata":{"id":"A1o075KvWvDF","executionInfo":{"status":"ok","timestamp":1734628837677,"user_tz":0,"elapsed":223,"user":{"displayName":"Mohammad Usaid Syed","userId":"11364143280426579494"}}},"id":"A1o075KvWvDF","execution_count":22,"outputs":[]},{"cell_type":"code","source":["for text, annotations in training_data:\n","    spans = annotations[\"entities\"]\n","    spans = sorted(spans, key=lambda span: span[0])\n","    for i in range(len(spans)):\n","        for j in range(i + 1, len(spans)):\n","            if spans[i][1] > spans[j][0]:  # Check for overlap\n","                print(f\"Overlapping spans found - {i}: {spans[i]} and {spans[j]} in text: {text}\")"],"metadata":{"id":"NSEGLga6WvGT","executionInfo":{"status":"ok","timestamp":1734628839274,"user_tz":0,"elapsed":226,"user":{"displayName":"Mohammad Usaid Syed","userId":"11364143280426579494"}}},"id":"NSEGLga6WvGT","execution_count":23,"outputs":[]},{"cell_type":"code","source":["for text, annotations in training_data:\n","    spans = annotations[\"entities\"]\n","    for span in spans:\n","        start, end, label = span\n","        span_text = text[start:end]\n","        # print(f\"Span: {span}, Text: '{span_text}'\")\n","\n","        # Check for leading/trailing whitespace\n","        if span_text != span_text.strip():\n","            print(f\"Warning: Span '{span_text}' contains leading/trailing whitespace.\")\n","\n","        # Check if the span is within valid bounds\n","        if start < 0 or end > len(text):\n","            print(f\"Error: Span {span} is out of text bounds.\")\n","\n","        # Check for duplicate spans\n","        if spans.count(span) > 1:\n","            print(f\"Duplicate span found: {span}\")"],"metadata":{"id":"iuShaqtpYFiY","executionInfo":{"status":"ok","timestamp":1734628840574,"user_tz":0,"elapsed":199,"user":{"displayName":"Mohammad Usaid Syed","userId":"11364143280426579494"}}},"id":"iuShaqtpYFiY","execution_count":24,"outputs":[]},{"cell_type":"code","source":["for i in range(len(training_data)):\n","    text = training_data[i][0]\n","    annotations = training_data[i][1]\n","    for span in annotations[\"entities\"]:\n","        start, end, label = span\n","        if text[start-1] in \".,!?()[]{}'\\\"\" or text[end:end+1] in \".,!?()[]{}'\\\"\":\n","            print(f\"Span adjacent to punctuation {i}: {span}, Text: '{text[start:end]}'\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5b3R96fmhJ5w","outputId":"616198c7-fa09-4d71-8ece-84dccb87333b","executionInfo":{"status":"ok","timestamp":1734617532831,"user_tz":0,"elapsed":355,"user":{"displayName":"Mohammad Usaid Syed","userId":"11364143280426579494"}}},"id":"5b3R96fmhJ5w","execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["Span adjacent to punctuation 102: (2953, 2958, 'Tags'), Text: 'actor'\n","Span adjacent to punctuation 676: (2923, 2931, 'Tags'), Text: 'homicide'\n","Span adjacent to punctuation 847: (3031, 3044, 'Tags'), Text: 'mental health'\n"]}]},{"cell_type":"markdown","source":["Display Spans"],"metadata":{"id":"ZhJ28vxuYoRZ"},"id":"ZhJ28vxuYoRZ"},{"cell_type":"code","source":["# Load a blank English model for custom annotations\n","nlp = spacy.blank(\"en\")\n","sample = training_data[6]\n","print(sample[1])\n","print(sample[0])\n","# Sample text with the specified entity annotation\n","text = sample[0]\n","entities = sample[1][\"entities\"]\n","# Create a Doc object\n","doc = nlp(text)\n","\n","# Manually set the entity with the specified span and label\n","doc.ents = [nlp.make_doc(text).char_span(ent[0], ent[1], label=ent[2]) for ent in entities]\n","\n","# Visualize the text using displacy\n","displacy.render(doc, style=\"ent\", jupyter=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":842},"id":"tehyPNN1Yip7","executionInfo":{"status":"ok","timestamp":1734628869250,"user_tz":0,"elapsed":1839,"user":{"displayName":"Mohammad Usaid Syed","userId":"11364143280426579494"}},"outputId":"0201a814-3d1f-4d62-a9c7-a34a35fb1de1"},"id":"tehyPNN1Yip7","execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["{'entities': [(24, 40, 'Killers'), (60, 73, 'Tags'), (75, 91, 'Killers'), (94, 107, 'Tags'), (150, 163, 'Tags'), (194, 203, 'Location'), (391, 403, 'Victim'), (433, 453, 'Tags'), (535, 544, 'Setting'), (633, 643, 'Tags'), (645, 661, 'Tags'), (891, 904, 'Tags'), (1107, 1123, 'Killers'), (1182, 1192, 'Tags'), (1723, 1729, 'Setting'), (1979, 1992, 'Location'), (1994, 2003, 'Location'), (2356, 2371, 'Characters'), (2408, 2421, 'Location'), (2574, 2578, 'Crime'), (2583, 2587, 'Crime'), (2659, 2663, 'Crime'), (2781, 2787, 'Setting')]}\n","is '90 day fiance' star geoffrey paschel revealing his full criminal past? geoffrey paschel s criminal past has been a central theme of this season s 90 day fiancé before the 90 days but is the tennessee resident divulging his entire criminal history to his russian love interest? paschel confessed on the show to serving time in prison more than twenty years ago for dealing drugs, telling varya malina, the woman he met through an international dating site, that he was a different man now. but that isn t the only legal trouble the knoxville resident s had. paschel was also indicted by a grand jury late last year for aggravated kidnapping, domestic assault, interference with emergency call and vandalism after a girlfriend alleged that he had violently grabbed her by the neck and slammed her head into the wall, according to court documents obtained by oxygen com. the reality star s criminal past and the current allegations against him even prompted some tlc fans to start a change org petition that had gathered nearly 5,000 signatures to date calling for the network to remove him from the show. geoffrey paschel photo knox county sheriff s office he has been accused of kidnapping, abuse, rape, child endangerment, dealing drugs, felony larceny, theft and battery, the petition reads. he is barred from ever entering canada paschel has proclaimed his innocence and to date remains a part of the show and even recently proposed to malina an offer she turned down because she said she wasn t sure she could trust him. i love him but i am not sure if i could trust him in everything, she said on the show. i am not ready malina said his drug conviction and an incident where he yelled at her friends on a visit to russia had given her pause. these are big issues in the future, she said. oxygen com reached out to paschel through social media to address the allegations but has not received a response. serving time behind bars paschel was arrested in the late 1990s in blount county, tennessee for possession of a schedule vi controlled substance, possession of a schedule i controlled substance and possession of a schedule ii controlled substance with intent to resell, according to court records obtained by in touch weekly. he pleaded guilty to the charges against him and was sentenced to 11 months and 29 days in jail, the outlet reported. marian o briant, public information officer for the blount county sheriff s office declined to provide details about the arrest other than to tell oxygen com that he had been in their county s jail a few times between 1997 and 2000. she said there was no record of him being in jail in the county after 2000. paschel initially kept his time in prison a secret from malina, but decided he needed to come clean while he was in russia visiting his girlfriend s hometown. i told varya s mother that she can trust me, but i ve been keeping a major secret\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">is '90 day fiance' star \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    geoffrey paschel\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Killers</span>\n","</mark>\n"," revealing his full \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    criminal past\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Tags</span>\n","</mark>\n","? \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    geoffrey paschel\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Killers</span>\n","</mark>\n"," s \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    criminal past\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Tags</span>\n","</mark>\n"," has been a central theme of this season s \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    90 day fiancé\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Tags</span>\n","</mark>\n"," before the 90 days but is the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    tennessee\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Location</span>\n","</mark>\n"," resident divulging his entire criminal history to his russian love interest? paschel confessed on the show to serving time in prison more than twenty years ago for dealing drugs, telling \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    varya malina\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Victim</span>\n","</mark>\n",", the woman he met through an \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    international dating\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Tags</span>\n","</mark>\n"," site, that he was a different man now. but that isn t the only legal trouble the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    knoxville\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Setting</span>\n","</mark>\n"," resident s had. paschel was also indicted by a grand jury late last year for aggravated \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    kidnapping\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Tags</span>\n","</mark>\n",", \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    domestic assault\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Tags</span>\n","</mark>\n",", interference with emergency call and vandalism after a girlfriend alleged that he had violently grabbed her by the neck and slammed her head into the wall, according to court documents obtained by oxygen com. the reality star s \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    criminal past\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Tags</span>\n","</mark>\n"," and the current allegations against him even prompted some tlc fans to start a change org petition that had gathered nearly 5,000 signatures to date calling for the network to remove him from the show. \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    geoffrey paschel\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Killers</span>\n","</mark>\n"," photo knox county sheriff s office he has been accused of \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    kidnapping\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Tags</span>\n","</mark>\n",", abuse, rape, child endangerment, dealing drugs, felony larceny, theft and battery, the petition reads. he is barred from ever entering canada paschel has proclaimed his innocence and to date remains a part of the show and even recently proposed to malina an offer she turned down because she said she wasn t sure she could trust him. i love him but i am not sure if i could trust him in everything, she said on the show. i am not ready malina said his drug conviction and an incident where he yelled at her friends on a visit to \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    russia\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Setting</span>\n","</mark>\n"," had given her pause. these are big issues in the future, she said. oxygen com reached out to paschel through social media to address the allegations but has not received a response. serving time behind bars paschel was arrested in the late 1990s in \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    blount county\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Location</span>\n","</mark>\n",", \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    tennessee\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Location</span>\n","</mark>\n"," for possession of a schedule vi controlled substance, possession of a schedule i controlled substance and possession of a schedule ii controlled substance with intent to resell, according to court records obtained by in touch weekly. he pleaded guilty to the charges against him and was sentenced to 11 months and 29 days in jail, the outlet reported. \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    marian o briant\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Characters</span>\n","</mark>\n",", public information officer for the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    blount county\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Location</span>\n","</mark>\n"," sheriff s office declined to provide details about the arrest other than to tell oxygen com that he had been in their county s jail a few times between \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    1997\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Crime</span>\n","</mark>\n"," and \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    2000\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Crime</span>\n","</mark>\n",". she said there was no record of him being in jail in the county after \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    2000\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Crime</span>\n","</mark>\n",". paschel initially kept his time in prison a secret from malina, but decided he needed to come clean while he was in \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    russia\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Setting</span>\n","</mark>\n"," visiting his girlfriend s hometown. i told varya s mother that she can trust me, but i ve been keeping a major secret</div></span>"]},"metadata":{}}]},{"cell_type":"markdown","source":["Checking for Valid and Invalid spans"],"metadata":{"id":"m3NoH9PkZRiM"},"id":"m3NoH9PkZRiM"},{"cell_type":"code","source":["nlp = spacy.load(\"en_core_web_sm\")\n","db = DocBin()  # Create a DocBin object\n","\n","for i in range(len(training_data)):  # Loop through the training data\n","  text, annot = training_data[i]\n","  doc = nlp.make_doc(text)  # Create a spaCy Doc object from the text\n","\n","  # Validate and add spans to the Doc\n","  valid_spans = []\n","  for start, end, label in annot['entities']:\n","      if 0 <= start < len(text) and 0 < end <= len(text):  # Ensure indices are within bounds\n","          span = doc.char_span(start, end, label=label, alignment_mode=\"strict\")\n","          if span:  # Ensure the span is valid\n","              valid_spans.append(span)\n","          else:\n","              print(f\"Skipping invalid span: {start}-{end} for label {i} {label}\")\n","\n","      else:\n","          print(f\"Skipping out-of-range span: {start}-{end} for label {label}\")\n"],"metadata":{"id":"CY9KxlHdZARm"},"id":"CY9KxlHdZARm","execution_count":null,"outputs":[]},{"cell_type":"code","source":["db = DocBin()  # Create a DocBin object\n","\n","for i in range(19040 ,19041):  # Loop through the training data\n","  text, annot = training_data[i]\n","  doc = nlp.make_doc(text)  # Create a spaCy Doc object from the text\n","  for token in doc:\n","    print(f\"Token: '{token.text}' | Start: {token.idx} | End: {token.idx + len(token)}\")\n","\n","  # Validate and add spans to the Doc\n","  valid_spans = []\n","  for start, end, label in annot['entities']:\n","      if 0 <= start < len(text) and 0 < end <= len(text):  # Ensure indices are within bounds\n","          span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n","          if span:  # Ensure the span is valid\n","              valid_spans.append(span)\n","              # print(f\"Valid span: {span} for label {label}\")\n","          else:\n","              print(f\"Skipping invalid span: {start}-{end} for label {i} {label}\")\n","\n","      else:\n","          print(f\"Skipping out-of-range span: {start}-{end} for label {label}\")"],"metadata":{"id":"713fuBOAZJYe"},"id":"713fuBOAZJYe","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **SAVING ANNOTATIONS**"],"metadata":{"id":"glEmm-oKYXg7"},"id":"glEmm-oKYXg7"},{"cell_type":"code","execution_count":61,"id":"0Z-0SQ9KX15F","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":356,"status":"ok","timestamp":1734617718715,"user":{"displayName":"Mohammad Usaid Syed","userId":"11364143280426579494"},"user_tz":0},"id":"0Z-0SQ9KX15F","outputId":"3c5693fa-ae74-4046-abe9-4c918a5de952"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training data saved to /content/drive/MyDrive/AndElementTask/And Elements/preprocessed_data_tags.csv\n"]}],"source":["# Save the training data\n","save_training_data_to_csv(training_data, drive_path + \"preprocessed_data.csv\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"collapsed_sections":["cLlq7yAmTOfP","dVPh_qneTTNh","R-KDMuhUUOaW","Pu-e_HEFW1HQ"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":5}